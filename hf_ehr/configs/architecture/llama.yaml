# @package _global_

# See Llama 3.1 technical report for details: https://arxiv.org/pdf/2407.21783
# Specifically, Table 3 for model sizes + hyperparameters

model:
  name: llama
  # Name/path to pass to HF.from_pretrained()
  hf_name: meta-llama/Meta-Llama-3.1-8B-Instruct
  # If TRUE, then keep pretrained weights
  is_keep_pretrained_weights: False
  # shared settings from Llama 3.1 techhnical paper 
  config_kwargs:
    rms_norm_eps: 1e-05
    rope_theta: 500_000

# Default trainer
trainer:
  optimizer:
    lr: 2e-4

  scheduler:
    num_warmup_steps: 40_000
    num_decay_steps: 4_000_000
    initial_lr: 1e-6
    final_lr: 1e-5