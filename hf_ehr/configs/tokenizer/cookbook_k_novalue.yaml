# @package _global_

data:
  tokenizer:
    name: CookbookTokenizer
    # TODO: Change this to the correct path eg: /cache/tokenizers/cookbook_novalue_39k/tokenizer_config.json
    path_to_config: "__PLACEHOLDER__"
    metadata:
      is_remap_numerical_codes_to_quantiles: False # If True, remap numerical codes to a bucketed range
      min_code_occurrence_count: 0 # Any code that occurs < `min_code_occurrence_count` times in the train dataset will be excluded
      keep_n_max_occurrence_codes: null # Keep only the top `keep_n_max_occurrence_codes` codes, sorted by occurrence count in train dataset
      excluded_vocabs: # Exclude all codes that are in these vocabularies
        - STANFORD_OBS
      is_add_visit_start: True # If True, add "VISIT START" token before the start of every visit
      is_add_visit_end: True # If True, add "VISIT END" token after every visit
      is_add_day_att: True # If True, add "DAY __" token for T <= 1080, "LONG TERM" token for T > 1080 (CEHR-GPT style)
      is_add_day_week_month_att: False # If True, add "DAY __" token for T < 7, "WEEK __" token for 7 <= T < 30, "MONTH __" for 30 <= T < 360, "LONG TERM" token for T > 360 (CEHR-BERT style)